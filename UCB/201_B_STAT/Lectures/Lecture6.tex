% TeX root = ../Main.tex

% First argument to \section is the title that will go in the table of contents. Second argument is the title that will be printed on the page.
\section[Lecture 6 (Sept 16) -- {\it Minimal Sufficiency}]{Lecture 6}


\begin{eb}
Let $X_i \sim N(\theta,1) i.i.d. i = 1,\dots,n$. Show that $T = \sum\limits_{i=1}^n X_i$ is a sufficient statistic for $\theta$.

\begin{proof}
    $f_\theta (x_1,\dots,x_n) = \prod_{i=1}^n f_\theta (x_i) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} e^{-\frac{(x_i - \theta)^2}{2}} = \frac{1}{(2\pi)^{n/2}} e^{-\frac{1}{2} \sum_{i=1}^n (x_i - \theta)^2}$
    $$
    = \bigg[\frac{1}{(2\pi)^{n/2}} e^{\frac{\sum\limits_{i=1}^n x_i^2}{2}}\bigg] \cdot e^{-\frac{n\theta^2}{2} + \theta \sum\limits_{i=1}^n x_i} =  h(x) \cdot g_\theta(T(x))
    $$
\end{proof}
\end{eb}

\subsection{Minimal Sufficiency} % (fold)
\label{sub:Minimal Sufficiency}

\begin{db}[Minimal Sufficiency]
 Suppose $T(X)$ is sufficient for $P = \{P_\theta : \theta \in \Omega\}$. 
For any other sufficient statistic $S(X)$, if we can always find a function $f$ such that 
$T = f(S)$, then $T$ is minimally sufficient.  

\smallskip
$T = f(S)$ means 
\\(i) the knowledge of $S$ implies the knowledge of $T$, and 
\\(ii) $T$ provides a greater reduction of data unless $f$ is one-to-one.
\end{db}

A $d$-parameter exponential family has pdf in the following form
\[
p(x,\theta) = h(x) \exp\!\left[ \sum_{i=1}^d \eta_i(\theta) T_i(x) - A(\theta) \right],
\]
which is of full rank if $\eta(\Theta) = \{\eta_1(\theta), \ldots, \eta_d(\theta)\}$ has 
non-empty interior in $\mathbb{R}^d$ and $T_1(x), \ldots, T_d(x)$ are linearly independent.  

\textbf{In a full rank exponential family, the natural sufficient statistic }
\[
T = (T_1, \ldots, T_d)
\]
\textbf{is minimally sufficient.}


\begin{eb}
Let $X_i \sim N(\theta,\sigma^2) i.i.d. i = 1,\dots,n$. 

\begin{solution}
    \begin{align*}
    f_{\mu,\sigma^2}(x_1,\dots,x_n) & = \prod_{i=1}^n f_{\mu,\sigma^2}(x_i) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x_i - \mu)^2}{2\sigma^2}} = \frac{1}{(2\pi)^{n/2}\sigma^n} e^{-\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2} \\
     &  = \dfrac{1}{(2\pi)^{n/2}\sigma^n} \cdot \exp\{\frac{\mu}{\sigma^2} \sum\limits_{i=1}^n x_i  - \frac{1}{2\sigma^2} \sum\limits_{i=1}^n x_i^2 - \dfrac{n\mu^2}{2\sigma^2}\}\\
    \end{align*}
    \vspace{-35pt}
    \begin{align*}
    \eta_1 (\theta) = \frac{\mu}{\sigma^2} \qquad& T_1 = \sum\limits_{i=1}^n x_i \\
    \eta_2 (\theta) = -\frac{1}{2\sigma^2} \qquad& T_2 = \sum\limits_{i=1}^n x_i^2 \\
    A(\theta) = \dfrac{n\mu^2}{2\sigma^2} \qquad& h(x) = \dfrac{1}{(2\pi)^{n/2}\sigma^n}
    \end{align*}
\end{solution}

\end{eb}
% subsection Minimal Sufficiency (end)


\subsection{Moments estimation} % (fold)
\label{sub:Moments estimation}

\begin{db}
    Suppose $\theta = (\theta_1, \ldots, \theta_k)$. For $j=1,\ldots,k$, define the $j^{th}$ moment
    \[
    \alpha_j \equiv \alpha_j(\theta) = E_\theta[X^j] 
    = \int x^j \, dF_\theta(x)
    \]
    and the $j^{th}$ sample moment 
    \[
    \hat{\alpha}_j = \frac{1}{n}\sum_{i=1}^n X_i^j.
    \]
    
    The method of moments estimator $\hat{\theta}_n$ is defined to be the value of $\theta$ such that
    \[
    \begin{aligned}
    \alpha_1(\hat{\theta}_n) &= \hat{\alpha}_1, \\
    \alpha_2(\hat{\theta}_n) &= \hat{\alpha}_2, \\
    &\;\;\vdots \\
    \alpha_k(\hat{\theta}_n) &= \hat{\alpha}_k.
    \end{aligned}
    \]
\end{db}

\begin{eb}[ Normal $\mu,\sigma^2$]
For normal distribution $N(\mu,\sigma^2)$, we have
\[
\alpha_1(\theta) = E[X] = \mu, \qquad \alpha_2(\theta) = E[X^2] = \mu^2 + \sigma^2.
\]
The method of moments estimators are
\[\hat{\mu} = \frac{1}{n}\sum_{i=1}^n X_i, \qquad \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n X_i^2 - \hat{\mu}^2.\]
\end{eb}

\textbf{MOM generalization:} Instead of using $\alpha_j(\theta) = E_\theta[X^j]$, we can consider 
\[
\alpha_j(\theta) = E_\theta[g(X)^j]
\]
and find $\hat{\theta}_n$ such that
\[
\alpha_j(\hat{\theta}_n) = \frac{1}{n}\sum_{i=1}^n g(X_i)^j, 
\qquad j=1,\ldots,n.
\]

\emph{Why do this?}
\begin{enumerate}
    \item 	Flexibility: Sometimes raw moments don't exist (e.g., Cauchy distribution has no mean/variance), or are not convenient to solve.
	\item	Efficiency: Choosing $g_j$ cleverly can give better estimators (lower variance).
	\item	Connection to GMM: The generalized method of moments (GMM) in econometrics formalizes this ideaâ€”use more (possibly redundant) moment conditions than parameters, and solve them optimally.
\end{enumerate}



% subsection Moments estimation (end)