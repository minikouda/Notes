% TeX root = ../Main.tex

% First argument to \section is the title that will go in the table of contents. Second argument is the title that will be printed on the page.
\section[Lecture 13 (Oct 23) -- {\it Pearson's Test and Bayesian Statistics}]{Lecture 13}

\subsection{Pearson's Test}
\label{sub:PT}

\paragraph{Review connection}\label{par:review-questions-link}
See \hyperref[sub:Application of Likelihood Ratio Test]{LRT practice} for more.

Multinomial model:
\[X = (X_1,X_2,\dots,X_k) \sim \text{Multinomial}(n,p_1,p_2,\dots,p_k)\]
where \(\sum_{i=1}^{k} p_i = 1\) and \(p_i \ge 0\) for all \(i\).

The pdf is:
\[f(x_1,x_2,\dots,x_k|p_1,p_2,\dots,p_k) = \dfrac{n!}{x_1! x_2! \dots x_k!} p_1^{x_1} p_2^{x_2} \dots p_k^{x_k}\]
where \(x_i \ge 0\) for all \(i\) and \(\sum_{i=1}^{k} x_i = n\).

MLE:
\[\hat p_i = \dfrac{X_i}{n}, \quad i=1,2,\dots,k\]

\begin{db}[Pearson's Chi-Squared Test]
We want to test:
\[H_0: p = p^0 = (p_1^0,p_2^0,\dots,p_k^0) \qquad H_1: p \neq p^0\]
The test statistic is:
\[T = \sum_{i=1}^k \dfrac{(X_i - n p_i^0)^2}{n p_i^0} = \sum_{i=1}^k \dfrac{(X_i - E_i)^2}{E_i}\]
Under \(H_0\), when \(n\) is large, 
\[T \overset{D}{\rightarrow} \chi^2_{k-1}\]
The test rejects \(H_0\) when \(T > \chi^2_{k-1,\alpha}\).
\end{db}

\begin{eb}[Poisson]
\[
H_0: X_1,X_2\dots,X_n \sim Poisson(\lambda) \qquad H_1: \text{not null}
\]

Construct $K$ categories, where category $i$ corresponds to observing $i-1$ events for $i=1,2,\dots,K-1$ and category $K$ corresponds to observing at least $K-1$ events. Let $O_i$ be the observed counts in category $i$ and let $E_i$ be the expected counts in category $i$ under $H_0$. Then the test statistic is:
\[
X^2 = \sum_{i=1}^K \frac{(O_i - E_i)^2}{E_i}
\]

In practice, usually we use at least 5 categories.
\begin{align*}
\{0\} :=&  i = 1\\
\{1\} :=&  i = 2\\
\vdots & \\
\{K-2\} :=&  i = K-1\\
\{K-1, K, K+1, \dots \} :=&  i = K
\end{align*}

\[
Y_j = \#\{x_i | x_i = j-1\} \text{ for } j = 1,2,\dots,K-1
\]
\[
Y_K = \#\{x_i | x_i \ge K-1\}
\]
\[
p_j(\lambda) = \begin{cases}
    e^{-\lambda} \frac{\lambda^{j-1}}{(j-1)!} & j = 1,2,\dots,K-1\\
    1 - \sum_{i=0}^{K-2} e^{-\lambda} \frac{\lambda^i}{i!} & j = K
\end{cases} 
\]

If $\lambda$ is known, then under $H_0$,
\[
T(X) = \sum\limits_{j=1}^{K} \dfrac{(Y_j - n p_j(\lambda))^2}{np_j(\lambda)} \overset{D}{\rightarrow} \chi^2_{K-1} 
\]

If $\lambda$ is unknown, then

we use the MLE $\hat \lambda = \bar X_n$ to estimate $\lambda$. Then under $H_0$,
\[
T(X) = \sum\limits_{j=1}^{K} \dfrac{(Y_j - n p_j(\lambda))^2}{np_j(\lambda)} \overset{D}{\rightarrow} \chi^2_{K-1-1} 
\]


\end{eb}


\subsection{Bayesian Statistics}

\[
f(\theta|x^n) = \dfrac{f(x^n|\theta)f(\theta)}{f(x^n)}
\]
\begin{enumerate}
    \item $f(\theta)$ is the prior distribution of $\theta$.
    \item $f(x^n|\theta)$ is the likelihood function.
    \item $f(\theta|x^n)$ is the posterior distribution of $\theta$ given data $x^n$.
    \item $f(x^n)$ is the marginal likelihood of the data, can be hard to compute. Serve as the normalizing constant.
\end{enumerate}

Good news: We often do not need to compute $f(x^n)$, since the family of prior and posterior distributions are often the same (conjugate prior).
\begin{eb}[Normal Model]
Suppose $X_1,X_2,\dots,X_n \overset{iid}{\sim} N(\mu,\sigma^2)$, and the prior distribution of $\mu$ is $N(\mu_0,\sigma_0^2)$, i.e.,
\[f(\mu) = \dfrac{1}{\sqrt{2\pi \sigma_0^2}} e^{-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}}\]    
\begin{solution}
    \[
    f(\mu|x_1,\dots,x_n) \propto f(x_1,\dots,x_n|\mu) f(\mu) \propto \left(\prod_{i=1}^n \dfrac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x_i - \mu)^2}{2\sigma^2}}\right) \dfrac{1}{\sqrt{2\pi \sigma_0^2}} e^{-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}}
    \]
    \[\propto \exp\{-\frac{1}{2}\left(\frac{n}{\sigma^2} + \frac{1}{\sigma_0^2}\right) \left(\mu - \frac{\frac{n\bar{x}}{\sigma^2} + \frac{\mu_0}{\sigma_0^2}}{\frac{n}{\sigma^2} + \frac{1}{\sigma_0^2}}\right)^2\} 
    \]
\end{solution}
\end{eb}

\begin{eb}[Poisson-Gamma Model]
Suppose $X_1,X_2,\dots,X_n \overset{iid}{\sim} Poisson(\theta)$, and the prior distribution of $\theta$ is $Gamma(\alpha,\beta)$, i.e.,
\[f(\theta) = \dfrac{\beta^\alpha}{\Gamma(\alpha)} \theta^{\alpha - 1} e^{-\beta \theta}, \theta > 0\]

\begin{solution}
    \[
    f(\theta|x_1,\dots,x_n) \propto f(x_1,\dots,x_n|\theta) f(\theta) \propto \left(\prod_{i=1}^n \dfrac{e^{-\theta} \theta^{x_i}}{x_i!}\right) \theta^{\alpha - 1} e^{-\beta \theta} \propto \theta^{(\sum_{i=1}^n x_i )+ \alpha - 1} e^{-(n + \beta) \theta}
    \]

    \[
    \E(\theta| X_1,\dots,X_n) = \dfrac{\sum\limits_{i=1}^{n} X_i + \alpha}{n+\beta} = \dfrac{n}{n+\beta} \bar X_n + \dfrac{\beta}{n + \beta} \dfrac{\alpha}{\beta}
    \]
\end{solution}
\end{eb}

\phantomsection
\label{example:pg}
\begin{eb}
For $X_1,\dots,X_n | \theta \overset{iid}{\sim} N(\theta,\sigma^2)$ , where $\sigma^2$ is known, $\theta \sim N(a,b^2)$
The posterior distribution is
\[\theta | x^n \sim N\left(\dfrac{\frac{n\bar{x}}{\sigma^2} + \frac{a}{b^2}}{\frac{n}{\sigma^2} + \frac{1}{b^2}}, \dfrac{1}{\frac{n}{\sigma^2} + \frac{1}{b^2}}\right)\]

The mean could be written as a weighted average of the prior mean and the sample mean:
\[\dfrac{\frac{n\bar{x}}{\sigma^2} + \frac{a}{b^2}}{\frac{n}{\sigma^2} + \frac{1}{b^2}} = \left(\dfrac{\frac{n}{\sigma^2}}{\frac{n}{\sigma^2} + \frac{1}{b^2}}\right) \bar{x} + \left(\dfrac{\frac{1}{b^2}}{\frac{n}{\sigma^2} + \frac{1}{b^2}}\right) a\]

When $n \rightarrow \infty$, the weight for the prior $\rightarrow 0$.
\end{eb}
\subsection{Posterior Inference}

In Bayesian statistics, all inference is based on the posterior distribution.
We can use the posterior to calculate quantities similar to those under
frequentist statistics (point estimates and intervals), or we can examine the
posterior probability of any event of interest.

The posterior mean is a commonly used point estimator:
\[
\mathbb{E}[\theta \mid X_1, \dots, X_n]
=
\int \theta \, f(\theta \mid X_1, \dots, X_n)\, d\theta.
\]

It can often be written as a weighted average of the prior mean and the MLE.
For example, in \hyperref[example:pg]{Example} on the previous page,
\[
\mathbb{E}[\theta \mid X_1, \dots, X_n]
=
\frac{b^2 \sum_{i=1}^n X_i + a \sigma^2}{n b^2 + \sigma^2}
=
\frac{n b^2}{n b^2 + \sigma^2}\,\bar X_n
+
\frac{\sigma^2}{n b^2 + \sigma^2}\, a.
\]

\subsection{Credible Intervals}

A $1-\alpha$ credible interval for $\theta$ (also called a posterior interval)
is an interval $C_n$ satisfying
\[
P(\theta \in C_n \mid X_1, \dots, X_n) = 1 - \alpha.
\]

Note a few differences compared to a confidence interval:

\begin{itemize}
    \item The probability statement is about $\theta$, not $C_n$. The interval
    $C_n$ is a function of $X_1, \dots, X_n$, which we are conditioning on in
    the probability statement.
    \item The statement is an equality. This is different from a frequentist
    interval, which puts a lower bound on the probability of coverage.
    \item The intervals constructed this way may or may not have good
    frequentist coverage rates.
\end{itemize}


\subsection{Types of Credible Intervals}

Note that $C_n$ is not uniquely defined. There are several popular methods for
finding such intervals.

A $1-\alpha$ equal-tail credible interval is an interval $(a,b)$ such that
\[
\int_{-\infty}^{a} f(\theta \mid x^n)\, d\theta
=
\int_{b}^{\infty} f(\theta \mid x^n)\, d\theta
=
\frac{\alpha}{2}.
\]

A $1-\alpha$ highest posterior density (HPD) region $R_n$ is defined such that:
\begin{enumerate}
    \item $P(\theta \in R_n \mid x^n) = 1 - \alpha$,
    \item $R_n = \{ \theta : f(\theta \mid x^n) > k \}$ for some constant $k$.
\end{enumerate}

When $f(\theta \mid x^n)$ is unimodal, $R_n$ is an interval.

Often it is more informative to plot $f(\theta \mid x^n)$ than it is to report
an interval.
